<html>
	<head>
		<title> infoliftinterest.html </title>
	</head>
	<body>
	  <strong>
	    What are f1, accurracy, precision and recall?
	  </strong>
	  <p>
	    Classification models are used in classification problems to predict the target class of the data sample. It is important to evaluate the performance of the classifications model in order to reliably use these models in production for solving real-world problems. These performance metrics include:
	  </p>
	  <ul>
	    <li>
	     <strong>Precision Score:</strong> measures the proportion of positively predicted labels that are actually correct. Precision is also known as the positive predictive value. Precision is used in conjunction with the recall to trade-off false positives and false negatives. Precision is affected by the class distribution. If there are more samples in the minority class, then precision will be lower. Mathematically, it represents the ratio of true positive to the sum of true positive and false positive: <strong>Precision Score = TP / (FP + TP)</strong>
	    </li>
	    <li>
	      <strong>Recall Score:</strong> represents the model’s ability to correctly predict the positives out of actual positives. The higher the recall score, the better the machine learning model is at identifying both positive and negative examples. Recall is also known as sensitivity or the true positive rate. Mathematically, it represents the ratio of true positive to the sum of true positive and false negative: <strong>Recall Score = TP / (FN + TP)</strong>
	    </li>
	    <li>
	      <strong>Accuracy Score:</strong> a machine learning classification model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations. In other words, accuracy tells us how often we can expect our machine learning model will correctly predict an outcome out of the total number of times it made predictions. Mathematically, it represents the ratio of the sum of true positive and true negatives out of all the predictions: <strong>Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)</strong>
	    </li>
	    <li>
	      <strong>F1 Score:</strong> a machine learning model performance metric that gives equal weight to both the Precision and Recall for measuring its performance in terms of accuracy, making it an alternative to Accuracy metrics (it doesn’t require us to know the total number of observations). Mathematically, it can be represented as a harmonic mean of precision and recall score: <strong>F1 Score = 2* Precision Score * Recall Score/ (Precision Score + Recall Score)</strong>
	    </li>
	  </ul>
	</body>
</html>